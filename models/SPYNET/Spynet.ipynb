{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa045b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aba1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "video_name = '/content/drive/MyDrive/Colab Notebooks/piv_video/re1000raw500frame2.avi' # or any other extension like .avi etc\n",
    "vidcap = cv2.VideoCapture(video_name)\n",
    "success,image = vidcap.read()\n",
    "count = 1\n",
    "while success:\n",
    "  cv2.imwrite(\"%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "  success,image = vidcap.read()\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4412f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getopt\n",
    "import math\n",
    "import numpy\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import sys\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "##########################################################\n",
    "\n",
    "assert(int(str('').join(torch.__version__.split('.')[0:2])) >= 13) # requires at least pytorch version 1.3.0\n",
    "\n",
    "torch.set_grad_enabled(False) # make sure to not compute gradients for computational performance\n",
    "\n",
    "torch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance\n",
    "\n",
    "##########################################################\n",
    "\n",
    "arguments_strModel = 'sintel-final' # 'sintel-final', or 'sintel-clean', or 'chairs-final', or 'chairs-clean', or 'kitti-final'\n",
    "arguments_strOne = '0.png'\n",
    "arguments_strTwo = '1.png'\n",
    "arguments_strOut = 'out.png'\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "backwarp_tenGrid = {}\n",
    "\n",
    "def backwarp(tenInput, tenFlow):\n",
    "    if str(tenFlow.shape) not in backwarp_tenGrid:\n",
    "        tenHor = torch.linspace(-1.0 + (1.0 / tenFlow.shape[3]), 1.0 - (1.0 / tenFlow.shape[3]), tenFlow.shape[3]).view(1, 1, 1, -1).repeat(1, 1, tenFlow.shape[2], 1)\n",
    "        tenVer = torch.linspace(-1.0 + (1.0 / tenFlow.shape[2]), 1.0 - (1.0 / tenFlow.shape[2]), tenFlow.shape[2]).view(1, 1, -1, 1).repeat(1, 1, 1, tenFlow.shape[3])\n",
    "\n",
    "        backwarp_tenGrid[str(tenFlow.shape)] = torch.cat([ tenHor, tenVer ], 1)\n",
    "    # end\n",
    "\n",
    "    tenFlow = torch.cat([ tenFlow[:, 0:1, :, :] / ((tenInput.shape[3] - 1.0) / 2.0), tenFlow[:, 1:2, :, :] / ((tenInput.shape[2] - 1.0) / 2.0) ], 1)\n",
    "\n",
    "    return torch.nn.functional.grid_sample(input=tenInput, grid=(backwarp_tenGrid[str(tenFlow.shape)] + tenFlow).permute(0, 2, 3, 1), mode='bilinear', padding_mode='border', align_corners=False)\n",
    "# end\n",
    "\n",
    "##########################################################\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        class Preprocess(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "            # end\n",
    "\n",
    "            def forward(self, tenInput):\n",
    "                tenInput = tenInput.flip([1])\n",
    "                tenInput = tenInput - torch.tensor(data=[0.485, 0.456, 0.406], dtype=tenInput.dtype, device=tenInput.device).view(1, 3, 1, 1)\n",
    "                tenInput = tenInput * torch.tensor(data=[1.0 / 0.229, 1.0 / 0.224, 1.0 / 0.225], dtype=tenInput.dtype, device=tenInput.device).view(1, 3, 1, 1)\n",
    "\n",
    "                return tenInput\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        class Basic(torch.nn.Module):\n",
    "            def __init__(self, intLevel):\n",
    "                super().__init__()\n",
    "\n",
    "                self.netBasic = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(in_channels=8, out_channels=32, kernel_size=7, stride=1, padding=3),\n",
    "                    torch.nn.ReLU(inplace=False),\n",
    "                    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=7, stride=1, padding=3),\n",
    "                    torch.nn.ReLU(inplace=False),\n",
    "                    torch.nn.Conv2d(in_channels=64, out_channels=32, kernel_size=7, stride=1, padding=3),\n",
    "                    torch.nn.ReLU(inplace=False),\n",
    "                    torch.nn.Conv2d(in_channels=32, out_channels=16, kernel_size=7, stride=1, padding=3),\n",
    "                    torch.nn.ReLU(inplace=False),\n",
    "                    torch.nn.Conv2d(in_channels=16, out_channels=2, kernel_size=7, stride=1, padding=3)\n",
    "                )\n",
    "            # end\n",
    "\n",
    "            def forward(self, tenInput):\n",
    "                return self.netBasic(tenInput)\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        self.netPreprocess = Preprocess()\n",
    "\n",
    "        self.netBasic = torch.nn.ModuleList([ Basic(intLevel) for intLevel in range(6) ])\n",
    "\n",
    "        self.load_state_dict({ strKey.replace('module', 'net'): tenWeight for strKey, tenWeight in torch.hub.load_state_dict_from_url(url='http://content.sniklaus.com/github/pytorch-spynet/network-' + arguments_strModel + '.pytorch', file_name='spynet-' + arguments_strModel).items() })\n",
    "    # end\n",
    "\n",
    "    def forward(self, tenOne, tenTwo):\n",
    "        tenFlow = []\n",
    "\n",
    "        tenOne = [ self.netPreprocess(tenOne) ]\n",
    "        tenTwo = [ self.netPreprocess(tenTwo) ]\n",
    "\n",
    "        for intLevel in range(1):\n",
    "            if tenOne[0].shape[2] > 128 or tenOne[0].shape[3] >  128:\n",
    "                tenOne.insert(0, torch.nn.functional.avg_pool2d(input=tenOne[0], kernel_size=2, stride=2, count_include_pad=False))\n",
    "                tenTwo.insert(0, torch.nn.functional.avg_pool2d(input=tenTwo[0], kernel_size=2, stride=2, count_include_pad=False))\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        tenFlow = tenOne[0].new_zeros([ tenOne[0].shape[0], 2, int(math.floor(tenOne[0].shape[2] / 2.0)), int(math.floor(tenOne[0].shape[3] / 2.0)) ])\n",
    "\n",
    "        for intLevel in range(len(tenOne)):\n",
    "            tenUpsampled = torch.nn.functional.interpolate(input=tenFlow, scale_factor=2, mode='bilinear', align_corners=True) * 2.0\n",
    "\n",
    "            if tenUpsampled.shape[2] != tenOne[intLevel].shape[2]: tenUpsampled = torch.nn.functional.pad(input=tenUpsampled, pad=[ 0, 0, 0, 1 ], mode='replicate')\n",
    "            if tenUpsampled.shape[3] != tenOne[intLevel].shape[3]: tenUpsampled = torch.nn.functional.pad(input=tenUpsampled, pad=[ 0, 1, 0, 0 ], mode='replicate')\n",
    "\n",
    "            tenFlow = self.netBasic[intLevel](torch.cat([ tenOne[intLevel], backwarp(tenInput=tenTwo[intLevel], tenFlow=tenUpsampled), tenUpsampled ], 1)) + tenUpsampled\n",
    "        # end\n",
    "\n",
    "        return tenFlow\n",
    "    # end\n",
    "# end\n",
    "\n",
    "netNetwork = None\n",
    "\n",
    "##########################################################\n",
    "\n",
    "def estimate(tenOne, tenTwo):\n",
    "    global netNetwork\n",
    "    if netNetwork is None:\n",
    "        netNetwork = Network().eval()\n",
    "\n",
    "\n",
    "    assert(tenOne.shape[1] == tenTwo.shape[1])\n",
    "    assert(tenOne.shape[2] == tenTwo.shape[2])\n",
    "\n",
    "    intWidth = tenOne.shape[2]\n",
    "    intHeight = tenOne.shape[1]\n",
    "\n",
    "\n",
    "    #assert(intWidth == 1368) # remember that there is no guarantee for correctness, comment this line out if you acknowledge this and want to continue\n",
    "    #assert(intHeight == 2280) # remember that there is no guarantee for correctness, comment this line out if you acknowledge this and want to continue\n",
    "\n",
    "    tenPreprocessedOne = tenOne.view(1, 3, intHeight, intWidth)\n",
    "    tenPreprocessedTwo = tenTwo.view(1, 3, intHeight, intWidth)\n",
    "\n",
    "\n",
    "    intPreprocessedWidth = int(math.floor(math.ceil(intWidth / 32.0) * 32.0))\n",
    "    intPreprocessedHeight = int(math.floor(math.ceil(intHeight / 32.0) * 32.0))\n",
    "\n",
    "\n",
    "    tenPreprocessedOne = torch.nn.functional.interpolate(input=tenPreprocessedOne, size=(intPreprocessedHeight, intPreprocessedWidth), mode='bilinear', align_corners=False)\n",
    "    tenPreprocessedTwo = torch.nn.functional.interpolate(input=tenPreprocessedTwo, size=(intPreprocessedHeight, intPreprocessedWidth), mode='bilinear', align_corners=False)\n",
    "\n",
    "    tenFlow = torch.nn.functional.interpolate(input=netNetwork(tenPreprocessedOne, tenPreprocessedTwo), size=(intHeight, intWidth), mode='bilinear', align_corners=False)\n",
    "\n",
    "    tenFlow[:, 0, :, :] *= float(intWidth) / float(intPreprocessedWidth)\n",
    "    tenFlow[:, 1, :, :] *= float(intHeight) / float(intPreprocessedHeight)\n",
    "\n",
    "    return tenFlow[0, :, :, :].cpu()\n",
    "# end\n",
    "\n",
    "##########################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step = 13\n",
    "    count1 = 1\n",
    "    count2 = 2\n",
    "    width = 1024\n",
    "    height = 591\n",
    "    tensor1 = np.zeros((2,width,height))\n",
    "    tensor2 = np.zeros((2,width,height))\n",
    "    from PIL import ImageOps\n",
    "    for i in range(1,30):\n",
    "        img = Image.open(f'{count1}.png')\n",
    "        imgGray1 = numpy.array(img)\n",
    "\n",
    "        img = Image.open(f'{count2}.png')\n",
    "        imgGray2 = numpy.array(img)\n",
    "\n",
    "        # Convert grayscale images to RGB\n",
    "        imgGray1_rgb = ImageOps.grayscale(Image.fromarray(imgGray1)).convert('RGB')\n",
    "        imgGray2_rgb = ImageOps.grayscale(Image.fromarray(imgGray2)).convert('RGB')\n",
    "\n",
    "        # Convert to float and normalize\n",
    "        imgGray1_rgb = torch.FloatTensor(numpy.array(imgGray1_rgb).astype(numpy.float32) * (1.0 / 255.0))\n",
    "        imgGray2_rgb = torch.FloatTensor(numpy.array(imgGray2_rgb).astype(numpy.float32) * (1.0 / 255.0))\n",
    "\n",
    "        # Add batch dimension\n",
    "        tenOne = imgGray1_rgb.unsqueeze(0)\n",
    "        tenTwo = imgGray2_rgb.unsqueeze(0)\n",
    "\n",
    "\n",
    "        # Process the tensors\n",
    "        tenOutput = estimate(tenOne, tenTwo)\n",
    "\n",
    "\n",
    "        max_x = np.max(tenOutput.numpy()[0,:,:])\n",
    "        norm_tensor_x = tenOutput.numpy()[0,:,:]/(max_x*32)\n",
    "        max_y = np.max(tenOutput.numpy()[1,:,:])\n",
    "        norm_tensor_y = tenOutput.numpy()[1,:,:]*(-1)/(max_y*32)\n",
    "        tensor1[0,:,:] +=norm_tensor_x\n",
    "        tensor1[1,:,:] +=norm_tensor_y\n",
    "        tensor5 = tenOutput.numpy()\n",
    "\n",
    "        count1 = count1 + 1\n",
    "        count2 = count2 + 1\n",
    "        print(count1)\n",
    "\n",
    "\n",
    "    tensor1 = tensor1\n",
    "    tensor2 = tensor1/29\n",
    "    print(tenOutput.shape)\n",
    "\n",
    "    from torchvision.utils import save_image\n",
    "    tenOutput1 = tenOutput.numpy()\n",
    "\n",
    "    plt.quiver(np.arange(0, height, step), np.arange(width, -1, -step),\n",
    "          tensor1[0,::step, ::step], tensor1[1,::step, ::step])\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(128, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088be881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import cv2\n",
    "mag_img, pha_img = cv2.cartToPolar(tensor5[1, ...], tensor5[0, ...])\n",
    "\n",
    "im = sns.heatmap(mag_img[:,:], cmap=\"jet\",cbar=True,annot_kws={\"size\": 16})\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches(64, 32)\n",
    "sns.set(font_scale=10)\n",
    "plt.savefig(\"mag.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
